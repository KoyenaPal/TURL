{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/local_koyena/anaconda3/envs/rome/lib/python3.9/site-packages (4.2.0)\r\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/local_koyena/anaconda3/envs/rome/lib/python3.9/site-packages (from gensim) (1.7.3)\r\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/local_koyena/anaconda3/envs/rome/lib/python3.9/site-packages (from gensim) (5.2.1)\r\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/local_koyena/anaconda3/envs/rome/lib/python3.9/site-packages (from gensim) (1.22.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "from tqdm import trange\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from data_loader.hybrid_data_loaders import *\n",
    "from data_loader.header_data_loaders import *\n",
    "from data_loader.CT_Wiki_data_loaders import *\n",
    "from data_loader.RE_data_loaders import *\n",
    "from data_loader.EL_data_loaders import *\n",
    "from model.configuration import TableConfig\n",
    "from model.model import HybridTableMaskedLM, HybridTableCER, TableHeaderRanking, HybridTableCT,HybridTableEL,HybridTableRE,BertRE\n",
    "from model.transformers import BertConfig,BertTokenizer, WEIGHTS_NAME, AdamW, get_linear_schedule_with_warmup\n",
    "from utils.util import *\n",
    "from baselines.row_population.metric import average_precision,ndcg_at_k\n",
    "from baselines.cell_filling.cell_filling import *\n",
    "from model import metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'CER': (TableConfig, HybridTableCER, BertTokenizer),\n",
    "    'CF' : (TableConfig, HybridTableMaskedLM, BertTokenizer),\n",
    "    'HR': (TableConfig, TableHeaderRanking, BertTokenizer),\n",
    "    'CT': (TableConfig, HybridTableCT, BertTokenizer),\n",
    "    'EL': (TableConfig, HybridTableEL, BertTokenizer),\n",
    "    'RE': (TableConfig, HybridTableRE, BertTokenizer),\n",
    "    'REBERT': (BertConfig, BertRE, BertTokenizer)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data directory, this will be used to load test data\n",
    "data_dir = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of entity: 926135\n",
      "remove because of empty title: 14206\n",
      "remove because count<2: 847401\n"
     ]
    }
   ],
   "source": [
    "config_name = \"configs/table-base-config_v2.json\"\n",
    "device = torch.device('cuda')\n",
    "# load entity vocab from entity_vocab.txt\n",
    "entity_vocab = load_entity_vocab(data_dir, ignore_bad_title=True, min_ent_count=2)\n",
    "entity_wikid2id = {entity_vocab[x]['wiki_id']:x for x in entity_vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "This notebook shows examples of how to using the model components and running evaluation of different tasks.\n",
    "* [Pretrained and Cell Filling](#cf)\n",
    "* [Entity Linking](#el)\n",
    "* [Column Type Classification](#ct)\n",
    "* [Relation Extraction](#re)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"cf\"></a>\n",
    "# Pretrained and CF\n",
    "Here we show how to use the pretrained model to get contextualized representation for a given input table. \n",
    "\n",
    "We use the cell filling task for demonstration as it does not need task-specific finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_class, model_class, _ = MODEL_CLASSES['CF']\n",
    "config = config_class.from_pretrained(config_name)\n",
    "config.output_attentions = True\n",
    "\n",
    "# For CF, we use the base HybridTableMaskedLM, and directly load the pretrained checkpoint\n",
    "checkpoint = \"checkpoint/pretrained/\"\n",
    "model = model_class(config, is_simple=True)\n",
    "checkpoint = torch.load(os.path.join(checkpoint, 'pytorch_model.bin'))\n",
    "model.load_state_dict(checkpoint)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "# load the module for cell filling baselines\n",
    "CF = cell_filling(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir,\"CF_test_data.json\"), 'r') as f:\n",
    "    dev_data = json.load(f)[:10]\n",
    "print('example for cell filling')\n",
    "display(dev_data[0])\n",
    "# the dataset here is the dataloader for pretraining. We use it to pass the config to construct the cell filling example\n",
    "dataset = WikiHybridTableDataset(data_dir,entity_vocab,max_cell=100, max_input_tok=350, max_input_ent=150, src=\"dev\", max_length = [50, 10, 10], force_new=False, tokenizer = None, mode=0)\n",
    "print('example of pretraining data')\n",
    "with open(os.path.join(data_dir, 'dev_tables.jsonl'), 'r') as f:\n",
    "    for line in f:\n",
    "        example = json.loads(line.strip())\n",
    "        break\n",
    "display(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example of converting an arbitrary table to input\n",
    "# Here we show an example for cell filling task\n",
    "# The input entites are entities in the subject column, we append [ENT_MASK] and use its representation to match with the candidate entities\n",
    "def CF_build_input(pgEnt, pgTitle, secTitle, caption, headers, core_entities, core_entities_text, entity_cand, config):\n",
    "    tokenized_pgTitle = config.tokenizer.encode(pgTitle, max_length=config.max_title_length, add_special_tokens=False)\n",
    "    tokenized_meta = tokenized_pgTitle+\\\n",
    "                    config.tokenizer.encode(secTitle, max_length=config.max_title_length, add_special_tokens=False)\n",
    "    if caption != secTitle:\n",
    "        tokenized_meta += config.tokenizer.encode(caption, max_length=config.max_title_length, add_special_tokens=False)\n",
    "    tokenized_headers = [config.tokenizer.encode(header, max_length=config.max_header_length, add_special_tokens=False) for header in headers]\n",
    "    input_tok = []\n",
    "    input_tok_pos = []\n",
    "    input_tok_type = []\n",
    "    tokenized_meta_length = len(tokenized_meta)\n",
    "    input_tok += tokenized_meta\n",
    "    input_tok_pos += list(range(tokenized_meta_length))\n",
    "    input_tok_type += [0]*tokenized_meta_length\n",
    "    header_span = []\n",
    "    for tokenized_header in tokenized_headers:\n",
    "        tokenized_header_length = len(tokenized_header)\n",
    "        header_span.append([len(input_tok), len(input_tok)+tokenized_header_length])\n",
    "        input_tok += tokenized_header\n",
    "        input_tok_pos += list(range(tokenized_header_length))\n",
    "        input_tok_type += [1]*tokenized_header_length\n",
    "\n",
    "    input_ent = [config.entity_wikid2id[pgEnt] if pgEnt!=-1 else 0]\n",
    "    input_ent_text = [tokenized_pgTitle[:config.max_cell_length]]\n",
    "    input_ent_type = [2]\n",
    "    \n",
    "    # core entities in the subject column\n",
    "    input_ent += [config.entity_wikid2id[entity] for entity in core_entities]\n",
    "    input_ent_text += [config.tokenizer.encode(entity_text, max_length=config.max_cell_length, add_special_tokens=False) if len(entity_text)!=0 else [] for entity_text in core_entities_text]\n",
    "    input_ent_type += [3]*len(core_entities)\n",
    "    \n",
    "    # append [ent_mask]\n",
    "    input_ent += [config.entity_wikid2id['[ENT_MASK]']]*len(core_entities)\n",
    "    input_ent_text += [[]]*len(core_entities)\n",
    "    input_ent_type += [4]*len(core_entities)\n",
    "\n",
    "    input_ent_cell_length = [len(x) if len(x)!=0 else 1 for x in input_ent_text]\n",
    "    max_cell_length = max(input_ent_cell_length)\n",
    "    input_ent_text_padded = np.zeros([len(input_ent_text), max_cell_length], dtype=int)\n",
    "    for i,x in enumerate(input_ent_text):\n",
    "        input_ent_text_padded[i, :len(x)] = x\n",
    "    assert len(input_ent) == 1+2*len(core_entities)\n",
    "\n",
    "    input_tok_mask = np.ones([1, len(input_tok), len(input_tok)+len(input_ent)], dtype=int)\n",
    "    input_tok_mask[0, header_span[0][0]:header_span[0][1], len(input_tok)+1+len(core_entities):] = 0\n",
    "    input_tok_mask[0, header_span[1][0]:header_span[1][1], len(input_tok)+1:len(input_tok)+1+len(core_entities)] = 0\n",
    "    input_tok_mask[0, :, len(input_tok)+1+len(core_entities):] = 0\n",
    "\n",
    "    # build the mask for entities\n",
    "    input_ent_mask = np.ones([1, len(input_ent), len(input_tok)+len(input_ent)], dtype=int)\n",
    "    input_ent_mask[0, 1:1+len(core_entities), header_span[1][0]:header_span[1][1]] = 0\n",
    "    input_ent_mask[0, 1:1+len(core_entities), len(input_tok)+1+len(core_entities):] = np.eye(len(core_entities), dtype=int)\n",
    "    input_ent_mask[0, 1+len(core_entities):, header_span[0][0]:header_span[0][1]] = 0\n",
    "    input_ent_mask[0, 1+len(core_entities):, len(input_tok)+1:len(input_tok)+1+len(core_entities)] = np.eye(len(core_entities), dtype=int)\n",
    "    input_ent_mask[0, 1+len(core_entities):, len(input_tok)+1+len(core_entities):] = np.eye(len(core_entities), dtype=int)\n",
    "\n",
    "    input_tok_mask = torch.LongTensor(input_tok_mask)\n",
    "    input_ent_mask = torch.LongTensor(input_ent_mask)\n",
    "\n",
    "    input_tok = torch.LongTensor([input_tok])\n",
    "    input_tok_type = torch.LongTensor([input_tok_type])\n",
    "    input_tok_pos = torch.LongTensor([input_tok_pos])\n",
    "    \n",
    "    input_ent = torch.LongTensor([input_ent])\n",
    "    input_ent_text = torch.LongTensor([input_ent_text_padded])\n",
    "    input_ent_cell_length = torch.LongTensor([input_ent_cell_length])\n",
    "    input_ent_type = torch.LongTensor([input_ent_type])\n",
    "\n",
    "    input_ent_mask_type = torch.zeros_like(input_ent)\n",
    "    input_ent_mask_type[:,1+len(core_entities):] = config.entity_wikid2id['[ENT_MASK]']\n",
    "\n",
    "    candidate_entity_set = [config.entity_wikid2id[entity] for entity in entity_cand]\n",
    "    candidate_entity_set = torch.LongTensor([candidate_entity_set])\n",
    "    \n",
    "\n",
    "    return input_tok, input_tok_type, input_tok_pos, input_tok_mask,\\\n",
    "            input_ent, input_ent_text, input_ent_cell_length, input_ent_type, input_ent_mask_type, input_ent_mask, candidate_entity_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for table_id,pgEnt,pgTitle,secTitle,caption,(h1, h2),data_sample in tqdm(dev_data):\n",
    "    result = []\n",
    "    while len(data_sample)!=0:\n",
    "        core_entities = []\n",
    "        core_entities_text = []\n",
    "        target_entities = []\n",
    "        all_entity_cand = set()\n",
    "        entity_cand = []\n",
    "        for (core_e, core_e_text), target_e in data_sample[:100]:\n",
    "            assert target_e in entity_wikid2id\n",
    "            core_entities.append(core_e)\n",
    "            core_entities_text.append(core_e_text)\n",
    "            target_entities.append(target_e)\n",
    "            cands = CF.get_cand_row(core_e, h2)\n",
    "            cands = {key:value for key,value in cands.items() if key in entity_wikid2id}\n",
    "            entity_cand.append(cands)\n",
    "            all_entity_cand |= set(cands.keys()) \n",
    "        all_entity_cand = list(all_entity_cand)\n",
    "        input_tok, input_tok_type, input_tok_pos, input_tok_mask,\\\n",
    "            input_ent, input_ent_text, input_ent_text_length, input_ent_type, input_ent_mask_type, input_ent_mask, \\\n",
    "            candidate_entity_set = CF_build_input(pgEnt, pgTitle, secTitle, caption, [h1, h2], core_entities, core_entities_text, all_entity_cand, dataset)\n",
    "        input_tok = input_tok.to(device)\n",
    "        input_tok_type = input_tok_type.to(device)\n",
    "        input_tok_pos = input_tok_pos.to(device)\n",
    "        input_tok_mask = input_tok_mask.to(device)\n",
    "        input_ent_text = input_ent_text.to(device)\n",
    "        input_ent_text_length = input_ent_text_length.to(device)\n",
    "        input_ent = input_ent.to(device)\n",
    "        input_ent_type = input_ent_type.to(device)\n",
    "        input_ent_mask_type = input_ent_mask_type.to(device)\n",
    "        input_ent_mask = input_ent_mask.to(device)\n",
    "        candidate_entity_set = candidate_entity_set.to(device)\n",
    "        with torch.no_grad():\n",
    "            tok_outputs, ent_outputs = model(input_tok, input_tok_type, input_tok_pos, input_tok_mask,\n",
    "                            input_ent_text, input_ent_text_length, input_ent_mask_type,\n",
    "                            input_ent, input_ent_type, input_ent_mask, candidate_entity_set)\n",
    "            num_sample = len(target_entities)\n",
    "            ent_prediction_scores = ent_outputs[0][0,num_sample+1:].tolist()\n",
    "        for i, target_e in enumerate(target_entities):\n",
    "            predictions = ent_prediction_scores[i]\n",
    "            if len(entity_cand[i]) == 0:\n",
    "                result.append([target_e, entity_cand[i], [], []])\n",
    "            else:\n",
    "                tmp_cand_scores = []\n",
    "                for j, cand_e in enumerate(all_entity_cand):\n",
    "                    if cand_e in entity_cand[i]:\n",
    "                        tmp_cand_scores.append([cand_e, predictions[j]])\n",
    "                sorted_cand_scores =  sorted(tmp_cand_scores, key=lambda z:z[1], reverse=True)\n",
    "                sorted_cands = [z[0] for z in sorted_cand_scores]\n",
    "                # use H2H as baseline\n",
    "                base_sorted_cands = CF.rank_cand_h2h(h2, entity_cand[i])\n",
    "                result.append([target_e, entity_cand[i], sorted_cands, base_sorted_cands])\n",
    "        data_sample = data_sample[100:]\n",
    "    results.append({\n",
    "        'pgTitle': pgTitle,\n",
    "        'secTitle': secTitle,\n",
    "        'caption': caption,\n",
    "        'headers': [h1, h2],\n",
    "        'result': result\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('tok(metadata) outputs', len(tok_outputs))\n",
    "print('tok prediction logits: [batch_size, num_toks, vocab_size]\\n', tok_outputs[0].shape)\n",
    "print('tok hidden states: [batch_size, num_toks, hidden_size]\\n', tok_outputs[1].shape)\n",
    "print('tok attention: n_layers*[batch_size, num_attention_headers, num_toks, num_toks+num_ents]\\n', tok_outputs[2][0].shape)\n",
    "print('entity(cell) outputs', len(ent_outputs))\n",
    "print('ent prediction logits: [batch_size, num_ents, candidate_size]\\n', ent_outputs[0].shape)\n",
    "print('ent hidden states: [batch_size, num_ents, hidden_size]\\n', ent_outputs[1].shape)\n",
    "print('ent attention: n_layers*[batch_size, num_attention_headers, num_ents, num_toks+num_ents]\\n', ent_outputs[2][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(result):\n",
    "    recall = 0\n",
    "    precision_neural = [0, 0, 0, 0]\n",
    "    precision_base = [0, 0, 0, 0]\n",
    "    for target_e, cand, p_neural, p_base in result:\n",
    "        if target_e in cand:\n",
    "            recall += 1\n",
    "            if target_e == p_neural[0]:\n",
    "                precision_neural[0] += 1\n",
    "            if target_e == p_base[0]:\n",
    "                precision_base[0] += 1\n",
    "            if target_e in p_neural[:3]:\n",
    "                precision_neural[1] += 1\n",
    "            if target_e in p_neural[:5]:\n",
    "                precision_neural[2] += 1\n",
    "            if target_e in p_neural[:10]:\n",
    "                precision_neural[3] += 1\n",
    "            if target_e in p_base[:3]:\n",
    "                precision_base[1] += 1\n",
    "            if target_e in p_base[:5]:\n",
    "                precision_base[2] += 1\n",
    "            if target_e in p_base[:10]:\n",
    "                precision_base[3] += 1\n",
    "    if recall != 0:\n",
    "        return recall/len(result), [z/recall for z in precision_neural], [z/recall for z in precision_base]\n",
    "    else:\n",
    "        return 0, [0 for z in precision_neural], [0 for z in precision_base]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = [get_precision(x['result']) for x in results]\n",
    "print('recall', np.mean([x[0] for x in final_results]))\n",
    "print('neural')\n",
    "print('p@1', np.mean([x[1][0] for x in final_results if x[0]!=0]))\n",
    "print('p@3', np.mean([x[1][1] for x in final_results if x[0]!=0]))\n",
    "print('p@5', np.mean([x[1][2] for x in final_results if x[0]!=0]))\n",
    "print('p@10', np.mean([x[1][3] for x in final_results if x[0]!=0]))\n",
    "print('base')\n",
    "print('p@1', np.mean([x[2][0] for x in final_results if x[0]!=0]))\n",
    "print('p@3', np.mean([x[2][1] for x in final_results if x[0]!=0]))\n",
    "print('p@5', np.mean([x[2][2] for x in final_results if x[0]!=0]))\n",
    "print('p@10', np.mean([x[2][3] for x in final_results if x[0]!=0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = [get_precision(x['result']) for x in results]\n",
    "print('recall', np.mean([x[0] for x in final_results]))\n",
    "print('neural')\n",
    "print('p@1', np.mean([x[1][0] for x in final_results if x[0]!=0]))\n",
    "print('p@3', np.mean([x[1][1] for x in final_results if x[0]!=0]))\n",
    "print('p@5', np.mean([x[1][2] for x in final_results if x[0]!=0]))\n",
    "print('p@10', np.mean([x[1][3] for x in final_results if x[0]!=0]))\n",
    "print('base')\n",
    "print('p@1', np.mean([x[2][0] for x in final_results if x[0]!=0]))\n",
    "print('p@3', np.mean([x[2][1] for x in final_results if x[0]!=0]))\n",
    "print('p@5', np.mean([x[2][2] for x in final_results if x[0]!=0]))\n",
    "print('p@10', np.mean([x[2][3] for x in final_results if x[0]!=0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('neural')\n",
    "print('p@1', np.mean([x[1][0] for i,x in enumerate(final_results) if x[0]!=0 and 'team' not in results[i]['headers'][1]]))\n",
    "print('p@3', np.mean([x[1][1] for i,x in enumerate(final_results) if x[0]!=0 and 'team' not in results[i]['headers'][1]]))\n",
    "print('p@5', np.mean([x[1][2] for i,x in enumerate(final_results) if x[0]!=0 and 'team' not in results[i]['headers'][1]]))\n",
    "print('p@10', np.mean([x[1][3] for i,x in enumerate(final_results) if x[0]!=0 and 'team' not in results[i]['headers'][1]]))\n",
    "print('base')\n",
    "print('p@1', np.mean([x[2][0] for i,x in enumerate(final_results) if x[0]!=0 and 'team' not in results[i]['headers'][1]]))\n",
    "print('p@3', np.mean([x[2][1] for i,x in enumerate(final_results) if x[0]!=0 and 'team' not in results[i]['headers'][1]]))\n",
    "print('p@5', np.mean([x[2][2] for i,x in enumerate(final_results) if x[0]!=0 and 'team' not in results[i]['headers'][1]]))\n",
    "print('p@10', np.mean([x[2][3] for i,x in enumerate(final_results) if x[0]!=0 and 'team' not in results[i]['headers'][1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('neural')\n",
    "print('p@1', np.mean([x[1][0] for i,x in enumerate(final_results) if x[0]!=0 and 'team' not in results[i]['headers'][1]]))\n",
    "print('p@3', np.mean([x[1][1] for i,x in enumerate(final_results) if x[0]!=0 and 'team' not in results[i]['headers'][1]]))\n",
    "print('p@5', np.mean([x[1][2] for i,x in enumerate(final_results) if x[0]!=0 and 'team' not in results[i]['headers'][1]]))\n",
    "print('p@10', np.mean([x[1][3] for i,x in enumerate(final_results) if x[0]!=0 and 'team' not in results[i]['headers'][1]]))\n",
    "print('base')\n",
    "print('p@1', np.mean([x[2][0] for i,x in enumerate(final_results) if x[0]!=0 and 'team' not in results[i]['headers'][1]]))\n",
    "print('p@3', np.mean([x[2][1] for i,x in enumerate(final_results) if x[0]!=0 and 'team' not in results[i]['headers'][1]]))\n",
    "print('p@5', np.mean([x[2][2] for i,x in enumerate(final_results) if x[0]!=0 and 'team' not in results[i]['headers'][1]]))\n",
    "print('p@10', np.mean([x[2][3] for i,x in enumerate(final_results) if x[0]!=0 and 'team' not in results[i]['headers'][1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"el\"></a>\n",
    "# EL\n",
    "Evaluate Entity Linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dbpedia types from depedia_type_vocab.txt (now just type vocab)\n",
    "type_vocab = load_type_vocab(data_dir)\n",
    "config_class, model_class, _ = MODEL_CLASSES['EL']\n",
    "config = config_class.from_pretrained(config_name)\n",
    "config.ent_type_vocab_size = len(type_vocab)\n",
    "config.mode=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['377509-5',\n",
       " '2000 Rugby League World Cup',\n",
       " 'Final',\n",
       " '',\n",
       " ['australia', 'position', 'new zealand'],\n",
       " [[[9, 0], 'Robbie Kearns'],\n",
       "  [[12, 0], 'Scott Hill'],\n",
       "  [[14, 0], 'Nathan Hindmarsh'],\n",
       "  [[7, 2], 'Craig Smith'],\n",
       "  [[11, 2], 'Stephen Kearney'],\n",
       "  [[1, 0], 'Mat Rogers'],\n",
       "  [[13, 0], 'Trent Barrett'],\n",
       "  [[10, 0], 'Gorden Tallis'],\n",
       "  [[5, 2], 'Henry Paul'],\n",
       "  [[16, 0], 'Jason Stevens'],\n",
       "  [[4, 0], 'Wendell Sailor'],\n",
       "  [[17, 0], 'Chris Anderson'],\n",
       "  [[15, 2], 'Nathan Cayless'],\n",
       "  [[8, 2], 'Richard Swain'],\n",
       "  [[9, 2], 'Quentin Pongia'],\n",
       "  [[6, 2], 'Stacey Jones'],\n",
       "  [[2, 2], 'Tonie Carroll'],\n",
       "  [[11, 0], 'Bryan Fletcher'],\n",
       "  [[0, 0], 'Darren Lockyer'],\n",
       "  [[4, 2], 'Lesley Vainikolo'],\n",
       "  [[0, 2], 'Richie Barnett'],\n",
       "  [[15, 0], 'Darren Britt'],\n",
       "  [[8, 0], 'Andrew Johns'],\n",
       "  [[14, 2], 'Joe Vagana'],\n",
       "  [[12, 2], 'Ruben Wiki'],\n",
       "  [[7, 0], 'Shane Webcke'],\n",
       "  [[5, 0], 'Brad Fittler'],\n",
       "  [[13, 2], 'Robbie Paul'],\n",
       "  [[10, 2], 'Matt Rua'],\n",
       "  [[2, 0], 'Adam MacDougall'],\n",
       "  [[16, 2], 'Logan Swann'],\n",
       "  [[17, 2], 'Frank Endacott'],\n",
       "  [[6, 0], 'Brett Kimmorley'],\n",
       "  [[1, 2], 'Nigel Vagana'],\n",
       "  [[3, 2], 'Willie Talau']],\n",
       " [['Robbie Kearns', 'Australian rugby league footballer', ['RugbyPlayer']],\n",
       "  ['Scott Hill', 'frontman of Fu Manchu', []],\n",
       "  ['Scott Hill', 'Wikipedia disambiguation page', []],\n",
       "  ['Scott Hill', 'Australian rugby league footballer', ['RugbyPlayer']],\n",
       "  ['Scott Hill', 'mountain in the United States of America', []],\n",
       "  ['Scott Hill', 'mountain in Utah, United States of America', []],\n",
       "  ['Scott Hill', 'mountain in Tennessee, United States of America', []],\n",
       "  ['Scott Hill', 'mountain in Connecticut, United States of America', []],\n",
       "  ['Scott Hill', 'mountain in Vermont, United States of America', []],\n",
       "  ['Scott Hill', 'mountain in Pennsylvania, United States of America', []],\n",
       "  ['Scott Hill', 'mountain in Australia', []],\n",
       "  ['Scott Hill',\n",
       "   'mountain in Hampshire County, Massachusetts, United States of America',\n",
       "   []],\n",
       "  ['Scott Hill', 'mountain in Wyoming, United States of America', []],\n",
       "  ['Scott Hill', 'mountain in Lincoln County, United States of America', []],\n",
       "  ['Scott Hill',\n",
       "   'mountain in Norfolk County, Massachusetts, United States of America',\n",
       "   []],\n",
       "  ['Scott Hill', 'mountain in Texas, United States of America', []],\n",
       "  ['Scott Hill', 'mountain in Wisconsin, United States of America', []],\n",
       "  ['Scott Hill',\n",
       "   'mountain in Placer County, California, United States of America',\n",
       "   []],\n",
       "  ['Scott Hill',\n",
       "   'building in West Virginia, United States',\n",
       "   ['HistoricPlace']],\n",
       "  ['Scott Hill', 'mountain in Missouri, United States of America', []],\n",
       "  ['Scott Hill', 'mountain in California, United States of America', []],\n",
       "  ['Scott Hill', 'mountain in Colorado, United States of America', []],\n",
       "  ['Scott Hill', 'mountain in North Dakota, United States of America', []],\n",
       "  ['Scott Hill', 'mountain in Massachusetts, United States of America', []],\n",
       "  ['Scott Hill', 'mountain in United States of America', []],\n",
       "  ['Scott Hill', None, []],\n",
       "  ['Scott Hill', 'mountain in South Australia, Australia', []],\n",
       "  ['Scott Hillenbrand', 'American director, actor and screenwriter', []],\n",
       "  ['Scott Hillman', 'Canadian ice hockey defenceman', ['IceHockeyPlayer']],\n",
       "  ['Scott Hiller', 'Lacrosse coach and player', []],\n",
       "  ['Scott Hilliar', None, []],\n",
       "  ['Scott Hill', 'hill in Canada', []],\n",
       "  ['Nathan Hindmarsh', 'Australian rugby league player', ['RugbyPlayer']],\n",
       "  ['Craig A. Smith', 'researcher', []],\n",
       "  ['Craig Smith', 'researcher', []],\n",
       "  ['Craig Smith', 'American ice hockey player', ['IceHockeyPlayer']],\n",
       "  ['Craig Smith', 'American basketball player', []],\n",
       "  ['Craig Smith', 'Wikimedia disambiguation page', []],\n",
       "  ['Craig Smith', 'Scottish rugby union player', ['RugbyPlayer']],\n",
       "  ['Craig Smith',\n",
       "   'New Zealand rugby league foorballer and coach',\n",
       "   ['RugbyPlayer']],\n",
       "  ['Craig Smith', 'psychologist', []],\n",
       "  ['Craig Smith', 'New Zealander cricketer', []],\n",
       "  ['Craig Smith', 'American conductor', ['owl#Thing']],\n",
       "  ['Craig Smith', 'Australian rugby league footballer', ['RugbyPlayer']],\n",
       "  ['Craig Smith', 'Australian footballer', ['AustralianRulesFootballPlayer']],\n",
       "  ['Craig Smith', 'Australian 1990s rugby league player', []],\n",
       "  ['Craig Smith', 'basketball coach', ['CollegeCoach']],\n",
       "  ['Craig Smith', 'association football player (1976-)', []],\n",
       "  [\"Craig Smith's Files (NAID 7409166)\",\n",
       "   \"series in the National Archives and Records Administration's holdings\",\n",
       "   []],\n",
       "  [\"Craig Smith's Files (NAID 7368820)\",\n",
       "   \"series in the National Archives and Records Administration's holdings\",\n",
       "   []],\n",
       "  ['Stephen Kearney',\n",
       "   'New Zealand rugby league footballer and coach',\n",
       "   ['RugbyPlayer']],\n",
       "  ['Stephen Kearney', None, []],\n",
       "  ['Stephen Kearney', 'researcher', []],\n",
       "  ['Mat Rogers',\n",
       "   'Australian rugby league and rugby union footballer',\n",
       "   ['RugbyPlayer']],\n",
       "  ['Trent Barrett',\n",
       "   'Australian rugby league footballer and coach',\n",
       "   ['RugbyPlayer']],\n",
       "  ['Trent Barreta', 'American professional wrestler', ['Wrestler']],\n",
       "  ['Gorden Tallis',\n",
       "   'Australian rugby league footballer and administrator',\n",
       "   ['RugbyPlayer']],\n",
       "  ['Henry Paul',\n",
       "   'New Zealand rugby league footballer, and rugby union footballer and coach',\n",
       "   ['RugbyPlayer']],\n",
       "  ['Henry Paul', 'Peerage person ID=406007', []],\n",
       "  ['Henry Paul', 'American musician', ['MusicalArtist']],\n",
       "  ['Henry Paul', None, []],\n",
       "  ['Henry Paulson', '74th United States Secretary of the Treasury', []],\n",
       "  ['Henry Paul Minchin', '(1826-1909)', []],\n",
       "  ['Henry Paul Coventry', 'Peerage person ID=179994', []],\n",
       "  ['Henry Paulet, 16th Marquess of Winchester', 'peer and business man', []],\n",
       "  ['Henry Paulet', 'died 1672', []],\n",
       "  ['Henry Paul Minchin', '(1889-1954)', []],\n",
       "  ['Henry Paul Dobbin', 'Peerage person ID=388777', []],\n",
       "  ['Henry Paul Minchin', 'Peerage person ID=339197', []],\n",
       "  ['Henry Paul Lindley Wood', '(1879-1886)', []],\n",
       "  [\"Henry Paul D'Isney Kenworthy\", '(1849-1874)', []],\n",
       "  ['Henry Paul Lefebvre', 'born 2008', []],\n",
       "  ['Henry Paul Hansen', 'American palynologist', ['Scientist']],\n",
       "  ['Henry Paul Talbot', None, []],\n",
       "  ['Henry Paul Edmond Caron', 'French painter', []],\n",
       "  ['Henry Paul Jordan', None, []],\n",
       "  ['Henry Paull', 'politician', ['OfficeHolder']],\n",
       "  ['Henry Paulyn Gillow', 'Peerage person ID=543078', []],\n",
       "  ['Henry Paul Mason', 'Peerage person ID=608850', []],\n",
       "  ['Henry Paulet', 'Wikimedia disambiguation page', []],\n",
       "  ['Henry Paul Talbot', 'scientific article published in August 1927', []],\n",
       "  ['HENRY PAUL TALBOT', 'scientific article published on 01 August 1927', []],\n",
       "  ['Henri Cartan', 'French mathematician', ['Scientist']],\n",
       "  ['Paul Channon, Baron Kelvedon',\n",
       "   'British politician and Cabinet Minister',\n",
       "   []],\n",
       "  ['Harry Powlett, 4th Duke of Bolton', 'British politician', []],\n",
       "  ['Henry Howard Paul', 'American actor', ['owl#Thing']],\n",
       "  ['Henry Paul', None, []],\n",
       "  ['Henry Paul Monaghan', None, []],\n",
       "  ['Jason Stevens', 'Australian rugby league footballer', ['RugbyPlayer']],\n",
       "  ['Wendell Sailor',\n",
       "   'Australian rugby union and rugby league footballer',\n",
       "   ['RugbyPlayer']],\n",
       "  ['Chris Anderson', 'American author and entrepreneur', ['Person']],\n",
       "  ['Chris Anderson', 'curator of TED', ['Person']],\n",
       "  ['Christopher D Anderson', 'researcher', []],\n",
       "  ['Chris Anderson', 'Wikimedia disambiguation page', []],\n",
       "  ['Chris Anderson', 'American jazz pianist', ['MusicalArtist']],\n",
       "  ['Chris Anderson', 'Australian high jumper', ['owl#Thing']],\n",
       "  ['Chris Anderson', None, []],\n",
       "  ['Chris Anderson', 'Scottish footballer', []],\n",
       "  ['Chris Anderson',\n",
       "   'Australian rugby league footballer and coach, and rugby union coach',\n",
       "   ['RugbyPlayer']],\n",
       "  ['Chris Anderson', 'professional golfer', ['GolfPlayer']],\n",
       "  ['Chris Anderson', 'baseball player (1992-)', ['BaseballPlayer']],\n",
       "  ['Chris Anderson',\n",
       "   'American trumpeter (Southside Johnny and the Asbury Jukes)',\n",
       "   ['owl#Thing']],\n",
       "  ['Chris Anderson', 'American politician', ['OfficeHolder']],\n",
       "  [\"Chris Anderson: Technology's long tail\", 'TED2004', []],\n",
       "  ['Chris Anderson: How web video powers global innovation',\n",
       "   'TEDGlobal 2010',\n",
       "   []],\n",
       "  [\"Chris Anderson: TED's nonprofit transition\", 'TED2002', []],\n",
       "  ['Chris Anderson admite plagiar a Wikipedia', 'Wikinews article', []],\n",
       "  ['Nathan Cayless',\n",
       "   'Australian rugby league footballer and coach',\n",
       "   ['RugbyPlayer']],\n",
       "  ['Richard Swain', 'New Zealand rugby league footballer', ['RugbyPlayer']],\n",
       "  ['Richard Swaine', 'German politician', []],\n",
       "  ['Quentin Pongia',\n",
       "   'New Zealand rugby league footballer and coach',\n",
       "   ['RugbyPlayer']],\n",
       "  ['Stacey Jones',\n",
       "   'New Zealand rugby league footballer and coach',\n",
       "   ['RugbyPlayer']],\n",
       "  ['Stacey Jones', 'Peerage person ID=519369', []],\n",
       "  ['Tonie Carroll',\n",
       "   'New Zealand-Australian rugby league footballer',\n",
       "   ['RugbyPlayer']],\n",
       "  ['Bryan Fletcher', 'Nordic combined skier', ['Skier']],\n",
       "  ['Bryan Fletcher', 'American football player', ['AmericanFootballPlayer']],\n",
       "  ['Bryan Fletcher', 'Wikipedia disambiguation page', []],\n",
       "  ['Bryan Fletcher', 'Australian rugby league footballer', ['RugbyPlayer']],\n",
       "  ['Darren Lockyer',\n",
       "   'Australian rugby league footballer and commentator',\n",
       "   ['RugbyPlayer']],\n",
       "  ['Lesley Vainikolo',\n",
       "   'Tongan-New Zealand rugby league and rugby union footballer',\n",
       "   ['RugbyPlayer']],\n",
       "  ['Richie Barnett', 'New Zealand rugby league footballer', ['RugbyPlayer']],\n",
       "  ['Richie Barnett',\n",
       "   'Jamaican-British rugby league footballer',\n",
       "   ['RugbyPlayer']],\n",
       "  ['Darren Britt', 'Australian rugby league footballer', ['RugbyPlayer']],\n",
       "  ['Andrew Johns', 'triathlete', []],\n",
       "  ['Andrew Johns', 'Australian rugby league footballer', []],\n",
       "  ['Andrew Johns',\n",
       "   'Green Party candidate in the 2019 United Kingdom General Election',\n",
       "   []],\n",
       "  ['Andrew Johnson',\n",
       "   'American politician, 17th President of the United States (in office from 1865 to 1869)',\n",
       "   []],\n",
       "  ['Andrew Johnson', 'English association football player', []],\n",
       "  ['Andrew Johnson', 'Wikimedia disambiguation page', []],\n",
       "  ['Andrew Johnston', 'Scottish singer', ['MusicalArtist']],\n",
       "  ['Andrew Johnson National Historic Site',\n",
       "   'National Historic Site in the United States',\n",
       "   ['Building']],\n",
       "  ['Andy Johnson', 'basketball player', []],\n",
       "  ['Andrew Johnstone Nash Christian', '(1884-1935)', []],\n",
       "  ['Andrew Johnstone, 4th of Elphinstone', 'Peerage person ID=281381', []],\n",
       "  ['Andrew Johnston', '(1835-1870)', []],\n",
       "  ['Andrew Johnston', 'golfer', ['GolfPlayer']],\n",
       "  ['Andrew Johnston', '(1769-1845)', []],\n",
       "  ['Andrew Jonston', 'accused of witchcraft', []],\n",
       "  ['Andrew Johnson', 'painting by Arthur Stumpf', []],\n",
       "  ['Andrew Johnson', None, []],\n",
       "  ['Andrew Johnson National Cemetery',\n",
       "   'National cemetery in Greeneville, Tennessee',\n",
       "   ['HistoricPlace']],\n",
       "  ['Andrew Johnston', 'American actor', ['owl#Thing']],\n",
       "  ['Andrew Johnston', '(1798-1862)', []],\n",
       "  ['Andrew Johnson', 'archaeologist', []],\n",
       "  ['Andrew Johnston', 'British politician', []],\n",
       "  ['Andrew Johnston', 'Scottish politician', []],\n",
       "  ['Andrew Johnson', 'Recipient of the Medal of Honor', ['MilitaryPerson']],\n",
       "  ['Andrew Johnston', 'died 1876', []],\n",
       "  ['Andrew Johnson', 'researcher ORCID ID = 0000-0002-7952-6536', []],\n",
       "  ['Andrew Johnston', 'born 1965', []],\n",
       "  ['Andrew Johnston', 'Peerage person ID=651693', []],\n",
       "  ['Andrew Johnson', 'painting by Washington Bogart Cooper', []],\n",
       "  ['Andrew Johnston', 'Peerage person ID=651456', []],\n",
       "  ['Andrew Johnstone', 'died 1810', []],\n",
       "  ['Andrew Johnston', 'Peerage person ID=712128', []],\n",
       "  ['Andrew Johnson', 'American architect', ['Architect']],\n",
       "  [\"Andrew Johnson's Third State of the Union Address\", None, []],\n",
       "  [\"Andrew Johnson's Fourth State of the Union Address\", None, []],\n",
       "  ['Andrew Johnson', None, []],\n",
       "  ['Andrew Johnston', 'American politician', []],\n",
       "  [\"Andrew Johnson's First State of the Union Address\", None, []],\n",
       "  [\"Andrew Johnson's Second State of the Union Address\", None, []],\n",
       "  ['Andy Dog Johnson', 'British artist', ['Artist']],\n",
       "  ['Andrew Johnson', None, []],\n",
       "  ['Andrew Johnston', 'died 1922', []],\n",
       "  ['Christopher Johnson', 'researcher', []],\n",
       "  ['Andrew Johnstone', 'Peerage person ID=199438', []],\n",
       "  ['Andrew Johnstone', None, []],\n",
       "  ['Andrew Johnson', 'American rower', []],\n",
       "  ['Andrew Johnson', None, []],\n",
       "  ['Andrew Johnston Watson', None, []],\n",
       "  ['Andrew Johnson', 'researcher', []],\n",
       "  ['Andrew Johnston', 'New Zealand poet', ['owl#Thing']],\n",
       "  ['Joe Vagana',\n",
       "   'New Zealand rugby league footballer and coach',\n",
       "   ['RugbyPlayer']],\n",
       "  ['Ruben Wiki', 'New Zealand rugby league footballer', ['RugbyPlayer']],\n",
       "  ['Shane Webcke', 'Australian rugby league footballer', ['RugbyPlayer']],\n",
       "  ['Brad Fittler',\n",
       "   'Australian rugby league footballer and coach',\n",
       "   ['RugbyPlayer']],\n",
       "  ['Robbie Paul',\n",
       "   'New Zealand rugby league footballer and administrator, and rugby union footballer',\n",
       "   ['RugbyPlayer']],\n",
       "  ['Matt Rua', 'New Zealand rugby league footballer', ['RugbyPlayer']],\n",
       "  ['Matt Ruatsale Homestead', None, ['Building']],\n",
       "  ['Adam MacDougall', 'American musician', ['MusicalArtist']],\n",
       "  ['Adam MacDougall', 'Australian rugby league footballer', ['RugbyPlayer']],\n",
       "  ['Logan Swann', 'New Zealand rugby league footballer', ['RugbyPlayer']],\n",
       "  ['Frank Endacott',\n",
       "   'New Zealand rugby league footballer and coach',\n",
       "   ['RugbyPlayer']],\n",
       "  ['Brett Kimmorley',\n",
       "   'Australian rugby league footballer and coach',\n",
       "   ['RugbyPlayer']],\n",
       "  ['Nigel Vagana', 'New Zealand rugby league footballer', ['RugbyPlayer']],\n",
       "  ['Willie Talau',\n",
       "   'Samoan-New Zealand rugby league footballer',\n",
       "   ['RugbyPlayer']]],\n",
       " [0,\n",
       "  3,\n",
       "  32,\n",
       "  39,\n",
       "  50,\n",
       "  53,\n",
       "  54,\n",
       "  56,\n",
       "  57,\n",
       "  88,\n",
       "  89,\n",
       "  98,\n",
       "  107,\n",
       "  108,\n",
       "  110,\n",
       "  111,\n",
       "  113,\n",
       "  117,\n",
       "  118,\n",
       "  119,\n",
       "  120,\n",
       "  122,\n",
       "  124,\n",
       "  173,\n",
       "  174,\n",
       "  175,\n",
       "  176,\n",
       "  177,\n",
       "  178,\n",
       "  181,\n",
       "  182,\n",
       "  183,\n",
       "  184,\n",
       "  185,\n",
       "  186],\n",
       " [[0],\n",
       "  [1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31],\n",
       "  [32],\n",
       "  [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n",
       "  [50, 51, 52],\n",
       "  [53],\n",
       "  [54, 55],\n",
       "  [56],\n",
       "  [57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87],\n",
       "  [88],\n",
       "  [89],\n",
       "  [90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106],\n",
       "  [107],\n",
       "  [108, 109],\n",
       "  [110],\n",
       "  [111, 112],\n",
       "  [113],\n",
       "  [114, 115, 116, 117],\n",
       "  [118],\n",
       "  [119],\n",
       "  [120, 121],\n",
       "  [122],\n",
       "  [123,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   127,\n",
       "   128,\n",
       "   129,\n",
       "   130,\n",
       "   131,\n",
       "   132,\n",
       "   133,\n",
       "   134,\n",
       "   135,\n",
       "   136,\n",
       "   137,\n",
       "   138,\n",
       "   139,\n",
       "   140,\n",
       "   141,\n",
       "   142,\n",
       "   143,\n",
       "   144,\n",
       "   145,\n",
       "   146,\n",
       "   147,\n",
       "   148,\n",
       "   149,\n",
       "   150,\n",
       "   151,\n",
       "   152,\n",
       "   153,\n",
       "   154,\n",
       "   155,\n",
       "   156,\n",
       "   157,\n",
       "   158,\n",
       "   159,\n",
       "   160,\n",
       "   161,\n",
       "   162,\n",
       "   163,\n",
       "   164,\n",
       "   165,\n",
       "   166,\n",
       "   167,\n",
       "   168,\n",
       "   169,\n",
       "   170,\n",
       "   171,\n",
       "   172],\n",
       "  [173],\n",
       "  [174],\n",
       "  [175],\n",
       "  [176],\n",
       "  [177],\n",
       "  [178, 179],\n",
       "  [180, 181],\n",
       "  [182],\n",
       "  [183],\n",
       "  [184],\n",
       "  [185],\n",
       "  [186]]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(os.path.join(data_dir, 'test_own.table_entity_linking.json'), 'r') as f:\n",
    "    example = json.load(f)[0]\n",
    "display(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try loading preprocessed data from data/procressed_EL/test_own.pickle\n"
     ]
    }
   ],
   "source": [
    "# load test data from [dataset].table_entity_linking.json\n",
    "test_dataset = ELDataset(data_dir, type_vocab, max_input_tok=500, src=\"test_own\", max_length = [50, 10, 10, 100], force_new=False, tokenizer = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for HybridTableEL:\n\tsize mismatch for table.embeddings.ent_embeddings.weight: copying a param with shape torch.Size([926135, 312]) from checkpoint, the shape in current model is torch.Size([404, 312]).\n\tsize mismatch for cand_embeddings.ent_type_embeddings.weight: copying a param with shape torch.Size([404, 312]) from checkpoint, the shape in current model is torch.Size([255, 312]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint/entity_linking/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/pytorch_model.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#checkpoint = torch.load(f\"checkpoint/pretrained/pytorch_model.bin\")\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/anaconda3/envs/rome/lib/python3.9/site-packages/torch/nn/modules/module.py:1482\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1477\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1478\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1479\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1482\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1483\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for HybridTableEL:\n\tsize mismatch for table.embeddings.ent_embeddings.weight: copying a param with shape torch.Size([926135, 312]) from checkpoint, the shape in current model is torch.Size([404, 312]).\n\tsize mismatch for cand_embeddings.ent_type_embeddings.weight: copying a param with shape torch.Size([404, 312]) from checkpoint, the shape in current model is torch.Size([255, 312])."
     ]
    }
   ],
   "source": [
    "model = model_class(config, is_simple=True)\n",
    "# load the checkpoint based on mode\n",
    "checkpoint = torch.load(f\"checkpoint/entity_linking/{config.mode}/pytorch_model.bin\")\n",
    "#checkpoint = torch.load(f\"checkpoint/pretrained/pytorch_model.bin\")\n",
    "model.load_state_dict(checkpoint)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 10\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = ELLoader(test_dataset, sampler=test_sampler, batch_size=test_batch_size, is_train=False)\n",
    "\n",
    "# Eval!\n",
    "print(\"Num examples = %d\"%len(test_dataset))\n",
    "print(\"Batch size = %d\"%test_batch_size)\n",
    "test_loss = 0.0\n",
    "test_acc = 0.0\n",
    "nb_test_steps = 0\n",
    "test_results = []\n",
    "\n",
    "for batch in tqdm(test_dataloader, desc=\"Evaluating\"):\n",
    "    table_id, input_tok, input_tok_type, input_tok_pos, input_tok_mask, \\\n",
    "            input_ent_text, input_ent_text_length, input_ent_type, input_ent_mask, \\\n",
    "            cand_name, cand_name_length,cand_description, cand_description_length,cand_type, cand_type_length, cand_mask, \\\n",
    "            labels,entities_index = batch\n",
    "    input_tok = input_tok.to(device)\n",
    "    input_tok_type = input_tok_type.to(device)\n",
    "    input_tok_pos = input_tok_pos.to(device)\n",
    "    input_tok_mask = input_tok_mask.to(device)\n",
    "    input_ent_text = input_ent_text.to(device)\n",
    "    input_ent_text_length = input_ent_text_length.to(device)\n",
    "    input_ent_type = input_ent_type.to(device)\n",
    "    input_ent_mask = input_ent_mask.to(device)\n",
    "    cand_name = cand_name.to(device)\n",
    "    cand_name_length = cand_name_length.to(device)\n",
    "    cand_description = cand_description.to(device)\n",
    "    cand_description_length = cand_description_length.to(device)\n",
    "    cand_type = cand_type.to(device)\n",
    "    cand_type_length = cand_type_length.to(device)\n",
    "    cand_mask = cand_mask.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    if config.mode == 1:\n",
    "        cand_description = None\n",
    "        cand_description_length = None\n",
    "    elif config.mode == 2:\n",
    "        cand_type = None\n",
    "        cand_type_length = None\n",
    "    elif config.mode != 0:\n",
    "        raise Exception\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tok, input_tok_type, input_tok_pos, input_tok_mask,\\\n",
    "            input_ent_text, input_ent_text_length, input_ent_type, input_ent_mask, \\\n",
    "            cand_name, cand_name_length,cand_description, cand_description_length,cand_type, cand_type_length, cand_mask, \\\n",
    "            labels)\n",
    "        loss = outputs[0]\n",
    "        prediction_scores = outputs[1]\n",
    "        predict_index = torch.argsort(prediction_scores.view(input_ent_text.size(0),input_ent_text.size(1)-1,-1),descending=True)\n",
    "        sorted_scores = (torch.gather(prediction_scores.view(input_ent_text.size(0),input_ent_text.size(1)-1,-1),-1,predict_index)).tolist()\n",
    "        predict_index = predict_index.tolist()\n",
    "        acc = metric.accuracy(prediction_scores, labels.view(-1),ignore_index=-1)\n",
    "        cand_length = cand_mask.sum(1).tolist()\n",
    "        ent_length = (labels!=-1).sum(1).tolist()\n",
    "        for i,t_id in enumerate(table_id):\n",
    "            test_results.append([t_id,entities_index[i],\\\n",
    "                                 [x[:cand_length[i]] for x in predict_index[i][:ent_length[i]]],\\\n",
    "                                 [x[:cand_length[i]] for x in sorted_scores[i][:ent_length[i]]],\\\n",
    "                                ])\n",
    "        test_loss += loss.mean().item()\n",
    "        test_acc += acc.item()\n",
    "    nb_test_steps += 1\n",
    "\n",
    "test_loss = test_loss / nb_test_steps\n",
    "test_acc = test_acc / nb_test_steps\n",
    "\n",
    "result = {\n",
    "    \"eval_loss\": test_loss,\n",
    "    \"eval_acc\": test_acc,\n",
    "}\n",
    "for key in sorted(result.keys()):\n",
    "    print(\"%s = %s\"%(key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we dump the predictions in seperate file an use another script for official evaluation.\n",
    "# The reason is that our entity linking is based on wikidata lookup. In certain cases, the candidates\n",
    "# do not contain the target entity, such test example is still considered for metric calculation.\n",
    "# However, since there is nothing to rank we do not pass thoses examples here. So the test examples here\n",
    "# is incomplete\n",
    "with open(os.path.join(data_dir,\"test_own_entity_linking_results_0.pkl\"),\"wb\") as f:\n",
    "    pickle.dump(test_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
